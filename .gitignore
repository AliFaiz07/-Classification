# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
cover/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
.pybuilder/
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
#   For a library or package, you might want to ignore these files since the code is
#   intended to run in multiple environments; otherwise, check them in:
# .python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
#Pipfile.lock

# poetry
#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control
#poetry.lock

# pdm
#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.
#pdm.lock
#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it
#   in version control.
#   https://pdm.fming.dev/#use-with-ide
.pdm.toml

# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# pytype static type analyzer
.pytype/

# Cython debug symbols
cython_debug/

# PyCharm
#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can
#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore
#  and can be added to the global gitignore or merged into this file.  For a more nuclear
#  option (not recommended) you can uncomment the following to ignore the entire idea folder.
#.idea/
# importing libraries
import pandas as pd
import numpy as np                     # For mathematical calculations
import seaborn as sns                  # For data visualization
import matplotlib.pyplot as plt
import seaborn as sns                  # For plotting graphs
%matplotlib inline
import warnings                        # To ignore any warnings
warnings.filterwarnings("ignore")

# loading the data
train = pd.read_csv("C:\\Users\\HP\Downloads\\Data_Science_Project_Problem_Statement (1)\\train.csv")
test = pd.read_csv("C:\\Users\\HP\\Downloads\\Data_Science_Project_Problem_Statement (1)\\test.csv")


train.columns


test.columns


train.shape, test.shape


# Print data types for each variable

train.dtypes

#printing first five rows of the dataset
train.head()



###univariate analysis

train['subscribed'].value_counts()


# plotting the bar plot of frequencies

train['subscribed'].value_counts().plot.bar()


sns.distplot(train["age"])

train['job'].value_counts().plot.bar()

train['default'].value_counts().plot.bar()


###bivariate analysis

print(pd.crosstab(train['job'],train['subscribed']))

job=pd.crosstab(train['job'],train['subscribed'])
job.div(job.sum(1).astype(float), axis=0).plot(kind="bar", stacked=True, figsize=(8,8))
plt.xlabel('Job')
plt.ylabel('Percentage')


print(pd.crosstab(train['default'],train['subscribed']))

default=pd.crosstab(train['default'],train['subscribed'])
default.div(default.sum(1).astype(float), axis=0).plot(kind="bar", stacked=True, figsize=(8,8))
plt.xlabel('default')
plt.ylabel('Percentage')


train['subscribed'].replace('no', 0,inplace=True)
train['subscribed'].replace('yes', 1,inplace=True)


corr = train.corr()
mask = np.array(corr)
mask[np.tril_indices_from(mask)] = False
fig,ax= plt.subplots()
fig.set_size_inches(20,10)
sns.heatmap(corr, mask=mask,vmax=.9, square=True,annot=True, cmap="YlGnBu")


train.isnull().sum()


###model building

target = train['subscribed']
train = train.drop('subscribed',1)


# applying dummies on the train dataset

train = pd.get_dummies(train)


from sklearn.model_selection import train_test_split

# splitting into train and validation with 20% data in validation set and 80% data in train set.

X_train, X_val, y_train, y_val = train_test_split(train, target, test_size = 0.2, random_state=12)


###logistic regression

from sklearn.linear_model import LogisticRegression

# defining the logistic regression model

lreg = LogisticRegression()


# fitting the model on  X_train and y_train

lreg.fit(X_train,y_train)


# making prediction on the validation set

prediction = lreg.predict(X_val)


from sklearn.metrics import accuracy_score



# calculating the accuracy score

accuracy_score(y_val, prediction)


###decesion tree

from sklearn.tree import DecisionTreeClassifier


# defining the decision tree model with depth of 4, you can tune it further to improve the accuracy score

clf = DecisionTreeClassifier(max_depth=4, random_state=0)


# fitting the decision tree model

clf.fit(X_train,y_train)


# making prediction on the validation set

predict = clf.predict(X_val)


# calculating the accuracy score

accuracy_score(y_val, predict)


test = pd.get_dummies(test)


test_prediction = clf.predict(test)


submission = pd.DataFrame()


# creating a Business_Sourced column and saving the predictions in it

submission['ID'] = test['ID']
submission['subscribed'] = test_prediction


submission['subscribed'].replace(0,'no',inplace=True)
submission['subscribed'].replace(1,'yes',inplace=True)


submission.to_csv("C:\\Users\\HP\\Downloads\\Data_Science_Project_Problem_Statement (1)\\test.csv", header=True, index=False)







